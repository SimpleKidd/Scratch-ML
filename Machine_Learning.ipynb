{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, roc_auc_score"
      ],
      "metadata": {
        "id": "QTEbExpxqnTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For Confusion Matrix**"
      ],
      "metadata": {
        "id": "Y0zjNdUVnmSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix(y_test,Predicted_class):\n",
        "  FP=0\n",
        "  TP=0\n",
        "  FN=0\n",
        "  TN=0\n",
        "  for j in range(len(y_test)):\n",
        "    if Predicted_class[j]==0:\n",
        "      if y_test[j]==0:\n",
        "        TN+=1\n",
        "      else:\n",
        "        FN+=1\n",
        "    else:\n",
        "      if Predicted_class[j]==1:\n",
        "        if y_test[j]==1:\n",
        "          TP+=1\n",
        "        else:\n",
        "          FP+=1\n",
        "    cm=np.array([[TN,FP],[FN,TP]])\n",
        "  return cm"
      ],
      "metadata": {
        "id": "UZ9TeGY8k1Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For Classwise Accuracy**"
      ],
      "metadata": {
        "id": "9GFuu6OEnzhG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iomwkKWL_Az"
      },
      "outputs": [],
      "source": [
        "def class_accuracy(y_test,model):\n",
        "  mat = confusion_matrix(y_test,model)\n",
        "  classacc = (((mat[0][0])/(mat[0][0]+mat[0][1]) + (mat[1][1])/(mat[1][1] + mat[1][0]))/2)*100\n",
        "  return classacc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For Decision Boundary**"
      ],
      "metadata": {
        "id": "9jqibxwRn4-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decbound(model,X,y,f1,f2):    # Made the function for decision boundary\n",
        "    x_, y_ = np.meshgrid(np.linspace(np.min(X[f1]), np.max(X[f1]), 100), np.linspace(np.min(X[f2]), np.max(X[f2]), 100)) #Created meshgrid\n",
        "    model.fit(X[[f1,f2]],y)          #Trained the model\n",
        "    Z = model.predict(pd.DataFrame(np.c_[x_.ravel(), y_.ravel()],columns = [f1,f2]))  #Predicted using the model\n",
        "    Z = Z.reshape(x_.shape)\n",
        "    plt.contourf(x_, y_, Z, cmap=plt.cm.RdYlBu)      #Did the contour plotting\n",
        "    sns.scatterplot(data=X[[f1,f2]],x = f1,y = f2,hue = y)   #Did the scatter plotting\n",
        "    return plt.show()"
      ],
      "metadata": {
        "id": "_V3hhL2Dk8sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For Storing Node of Decision Tree**"
      ],
      "metadata": {
        "id": "hqEYAHQboKg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class node:                                      #Creating an object to store data\n",
        "    def __init__(self, X, y, feature):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.feature = feature\n",
        "        self.childs = {}\n",
        "        self.decision = None\n",
        "\n",
        "    def isleaf(self):\n",
        "      if len(set(y))==1:\n",
        "        return 1\n",
        "      else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "GcFY3DCul1je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For DecisionTreeClassifier**(Gini Index as criteria)"
      ],
      "metadata": {
        "id": "IQIsoItnoQC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dtreeclassifier:       # Made an object for DecisionTreeClasifier\n",
        "    def __init__(self,X,y,max_depth=2):     #Made constructor to take inputs\n",
        "        self.root = None\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.max_depth = max_depth\n",
        "        \n",
        "    def gini(self,x):                      # For calculating gini index\n",
        "      uni,freq = np.unique(x,return_counts=True)\n",
        "      p = freq/len(x)\n",
        "      totprobab = np.sum(p ** 2)\n",
        "      self.ginindex = 1 - totprobab\n",
        "      return self.ginindex\n",
        "\n",
        "    def impurity(self,X,y,feature,splitter):     #For gini impurity cost which would be useful for finding best feature for split\n",
        "      Xlside,ylside,Xrside,yrside=[],[],[],[]\n",
        "      tot = len(X)\n",
        "      for i in X.index:\n",
        "        if X[feature][i] <= splitter:\n",
        "          Xlside.append(X[feature][i])\n",
        "          ylside.append(y[i])\n",
        "        else:\n",
        "          Xrside.append(X[feature][i])\n",
        "          yrside.append(y[i])\n",
        "      lside = len(Xlside)\n",
        "      rside = len(Xrside)\n",
        "      Xleft = pd.Series(Xlside,dtype = 'float64')\n",
        "      Xright = pd.Series(Xrside,dtype = 'float64')\n",
        "      yleft = pd.Series(ylside,dtype = 'float64')\n",
        "      yright = pd.Series(yrside,dtype = 'float64')\n",
        "      self.gini_impurity = (lside / tot)*self.gini(yleft) + (rside / tot)*self.gini(yright)  #It will also help in finding best split value\n",
        "      return self.gini_impurity \n",
        "\n",
        "    def fbest(self,X, y):       #Made function for selecting best feature for split\n",
        "      best = ''          #Initializing best feature as none\n",
        "      bestimpurity = float('inf')   #Initializing bestimpurity as big value\n",
        "      for i in X.columns:                    #Iterating over every feature\n",
        "          score = self.impurity(X, y, i, 0)       #Finding impurity w.r.t to feature i and splitvalue 0\n",
        "          if score < bestimpurity:\n",
        "              bestimpurity = score           #If condition satisfied updating best impurity as that score\n",
        "              best = i                       #Updating best feature value as one going on in loop\n",
        "      return best\n",
        "\n",
        "    def split(self,X,y,max_depth):   #For splitting the data recursively\n",
        "      if max_depth == 0 or len(X) == 0:\n",
        "          leaf = node(X,y,None)\n",
        "          return leaf\n",
        "      if gini(y) == 0:\n",
        "          leaf = node(X,y,None)\n",
        "          return leaf\n",
        "      f = self.fbest(X,y)\n",
        "      root = node(X,y,feature=f)\n",
        "      left = X[f] == 0\n",
        "      right = X[f] == 1  \n",
        "      if f in ['island', 'year']:\n",
        "          mid = X[f] == 2\n",
        "          root.childs['middle'] = self.split(X[mid],y[mid], max_depth-1)\n",
        "      if len(X[left]) > 0:\n",
        "          root.childs['left'] = self.split(X[left],y[left], max_depth-1)\n",
        "      if len(X[right]) > 0:\n",
        "          root.childs['right'] = self.split(X[right],y[right], max_depth-1)\n",
        "      return root \n",
        "\n",
        "    def train(self, X, y):                  #For Training the data\n",
        "      self.root = self.split(X, y, self.max_depth)  \n",
        "\n",
        "    def predict(self, x,s):                   #For prediction of data\n",
        "      if s.feature == None:\n",
        "        t = np.array(s.y)\n",
        "        t,freq = np.unique(t,return_counts  =True)\n",
        "        return t[np.argmax(freq)]\n",
        "      \n",
        "      column = []\n",
        "      for i in s.X.columns:\n",
        "        column += [i]\n",
        "      \n",
        "      index = column.index(s.feature)\n",
        "      if x[index] == 0:\n",
        "        return self.predict(x,s.childs['left'])\n",
        "      elif x[index] == 1:\n",
        "        return self.predict(x,s.childs['right'])\n",
        "      else:\n",
        "        return self.predict(x,s.childs['middle'])\n",
        "\n",
        "    def test(self,X):\n",
        "      o = np.array(X)\n",
        "      y_pred = []\n",
        "      for i in o:\n",
        "        ans = self.predict(i,self.root)\n",
        "        y_pred.append(ans)\n",
        "      return y_pred\n",
        "    \n",
        "    def accuracy(self,y_test,y_pred):\n",
        "      v = np.array(y_test)\n",
        "      c=0\n",
        "      for i in range(len(v)):\n",
        "        if v[i] == y_pred[i]:\n",
        "          c+=1\n",
        "      return c/len(y_test)"
      ],
      "metadata": {
        "id": "QNQY_73wk84T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For Continuos to Categorical**(Dataset specific)"
      ],
      "metadata": {
        "id": "Fmpfndpyn-4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cont_to_cat(X,y):       #Function to convert continuos data to categorical\n",
        "  f = ['bill_depth_mm','bill_length_mm','flipper_length_mm','body_mass_g']\n",
        "  split_store = []\n",
        "  for I in f:\n",
        "    n = (max(X[I])-min(X[I]))/2000     #Deciding the value to step in split values according to feature\n",
        "    splt = np.arange(min(X[I]),max(X[I]),n)     #Making array of split values \n",
        "    gini=[]\n",
        "    for i in splt:\n",
        "      gini.append(impurity(X,y,I,i))         #Making list of gini_impuruty according to feature value\n",
        "    s=splt[np.argmin(gini)]                   #Minimum gini will be best split so taking the index of that feature value\n",
        "    split_store.append(s)                \n",
        "    new = []                                 #Using that index finding the feature value for split\n",
        "    for i in X.index:\n",
        "      if X[I][i] <= s:\n",
        "        new.append(0)\n",
        "      else:\n",
        "        new.append(1)\n",
        "    X[I] = new\n",
        "  return X,split_store                  #Returning categorized data"
      ],
      "metadata": {
        "id": "bCA5briGk8xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Example of ROC-AUC Curve**"
      ],
      "metadata": {
        "id": "ZdYpg_wHocAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr,tpr,thresholds = roc_curve(y_test,y_pred)\n",
        "roc_auc = roc_auc_score(y_test,y_pred)  #computing roc_auc score\n",
        "print(roc_auc)\n",
        "\n",
        "plt.plot(fpr, tpr, label='ROC curve')   #plotting roc curve\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # random guess line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cf9hln_5k88q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**To discretize features**"
      ],
      "metadata": {
        "id": "rmYGu07DoiCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discreter(data, bins):                     #Creating function to Discretize the features\n",
        "  width = (max(data)+0.1 - min(data))/bins     #Computing Width u\n",
        "  bin = []                                     #Making list of bin edges\n",
        "  for i in range(bins+1):\n",
        "    z = min(data) + i*width\n",
        "    bin.append(z)                              #Appending in bin edges list\n",
        "  k=[]\n",
        "  for j in data:\n",
        "    for i in range(bins+1):\n",
        "      if(j>=bin[i] and j<bin[i+1]):          #Discretizing the continuos data using various conditional\n",
        "        k.append(i+1)\n",
        "        break\n",
        "    else:\n",
        "      if bin[bins+1]<=j:\n",
        "        k.append(bins+1)\n",
        "  return k"
      ],
      "metadata": {
        "id": "TPhjFhnrk9AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For Calculating Likelihood Probability**"
      ],
      "metadata": {
        "id": "JyD19Oc7oob7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def likelihood(data):        #Creating function for calculating Likelihood/Conditional probability\n",
        "  X = {}\n",
        "  for i in data.columns:\n",
        "    p = data[i].value_counts()    #counting unique values\n",
        "    l = p.index                   #Making list of unique values\n",
        "    d = {}                        #Creating Dictionary of Probability\n",
        "    for j in l:\n",
        "      prob = p[j]/len(data)\n",
        "      d[j] = prob\n",
        "    for k in range(1,26):\n",
        "      if k in d:\n",
        "        continue\n",
        "      else:\n",
        "        d[k] = 0\n",
        "    X[i] = d\n",
        "  return X  "
      ],
      "metadata": {
        "id": "44FDFeoUk9D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For Calculating Posterior Probability**(Data Specific)"
      ],
      "metadata": {
        "id": "vL_y4gtQowNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post(vertical,prior,bins):      #Made function to calculate posterior probability\n",
        "  evidence = 0\n",
        "  post1,post2,post3=[],[],[]\n",
        "  #prior is same for all there i.e 0.3333.\n",
        "  for j in range(1,bins+1):\n",
        "    for i in range(1,4):\n",
        "      evidence +=  likelihoods[i][vertical][j]*prior   #calculated evidence\n",
        "    l1 = likelihoods[1][vertical][j]                   #conditional probability of class1\n",
        "    l2 = likelihoods[2][vertical][j]                   #conditional probability of class2\n",
        "    l3 = likelihoods[3][vertical][j]                   #conditional probability of class3\n",
        "    p1 = (l1*prior)/evidence                           #Posterior probability of class1\n",
        "    p2 = (l2*prior)/evidence                           #Posterior probability of class2\n",
        "    p3 = (l3*prior)/evidence                           #Posterior probability of class3\n",
        "    post1.append(p1)\n",
        "    post2.append(p2)\n",
        "    post3.append(p3)\n",
        "  return post1,post2,post3"
      ],
      "metadata": {
        "id": "RAUBJIyTk9HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For GaussianNB Classifier**"
      ],
      "metadata": {
        "id": "RFxfGKt6o50j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianBayesClassifier():\n",
        "    def __init__(self, variant=1):\n",
        "        self.variant = variant\n",
        "    def train(self,X, y, variant=1):\n",
        "        self.classes, class_prior = np.unique(y, return_counts=True)  #Computing class priors\n",
        "        class_prior = class_prior / len(y)\n",
        "        self.class_priors = {}\n",
        "        k=0\n",
        "        for i in self.classes:\n",
        "          self.class_priors[i] = class_prior[k]\n",
        "          k+=1\n",
        "        self.means = {}                         # Computing class means and variances for each feature\n",
        "        self.variances = {}\n",
        "        for c in self.classes:\n",
        "          X_c = X[y == c]\n",
        "          self.means[c] = np.array(X_c.mean(axis=0))     \n",
        "          self.variances[c] = np.cov(X_c.T)\n",
        "          f = np.var(X,axis=0)[0]\n",
        "          if self.variant == 2:                 # Applying changes in variances according to variant.\n",
        "            self.variances[c] = np.cov(X.T)\n",
        "          elif self.variant == 3:\n",
        "            nf = len(X_c.columns)\n",
        "            self.variances[c] = f*(np.identity(nf))\n",
        "          \n",
        "\n",
        "        return self.classes, self.class_priors, self.means, self.variances\n",
        "\n",
        "    def test(self,X, classes, class_priors, means, variances):\n",
        "      self.pred = []\n",
        "      for i in X.index:\n",
        "        x5 = np.array(X.loc[i,:])\n",
        "        l = []\n",
        "        for j in classes:\n",
        "          probability = -0.5*(x5.T - means[j].T)@(np.linalg.inv(variances[j]))@(x5 - means[j]) - 0.5*np.log(np.linalg.det(variances[j])) + np.log(class_priors[j])\n",
        "          l.append(probability)\n",
        "        self.pred.append(classes[np.argmax(l)])\n",
        "      self.pred = np.array(self.pred)\n",
        "      return self.pred\n",
        "\n",
        "    def score(self,y):\n",
        "        cnt = 0\n",
        "        p = list(y)\n",
        "        for i in range(len(p)):\n",
        "          if p[i] == self.pred[i]:\n",
        "            cnt+=1\n",
        "        return cnt/len(y)\n",
        "    def split(self,w):\n",
        "     train=self.w.sample(frac=0.7, replace=False, random_state=1)\n",
        "     test=self.w.drop(train.index)\n",
        "     a=train.shape[0]\n",
        "     b=test.shape[0]\n",
        "\n",
        "     return (a,b)\n",
        "     \n",
        "    def decbound(self,X,y,f1,f2):\n",
        "      x_, y_ = np.meshgrid(np.linspace(np.min(X[f1]), np.max(X[f1]), 100), np.linspace(np.min(X[f2]), np.max(X[f2]), 100))\n",
        "      self.classes,self.class_priors,self.means, self.variances = self.train(X[[f1,f2]],y,self.variant)\n",
        "      Z = self.test(pd.DataFrame(np.c_[x_.ravel(), y_.ravel()],columns = [f1,f2]),self.classes,self.class_priors,self.means, self.variances)\n",
        "      Z = Z.reshape(x_.shape)\n",
        "      plt.contourf(x_, y_, Z, cmap=plt.cm.RdYlBu)\n",
        "      sns.scatterplot(data=X[[f1,f2]],x = f1,y = f2,hue = y)\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "mCJquogvk9Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For Bagging Classifier**"
      ],
      "metadata": {
        "id": "k9DGejQ6o_c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _Bagging_:\n",
        "  def __init__(self,n_estimators,samples):              #Initializing Input Variables and making them global.\n",
        "    self.n_estimators = n_estimators\n",
        "    self.samples = samples\n",
        "\n",
        "  def fit(self,X,y):                                   #Used for training the model of Training Datasets.\n",
        "    self.X,self.y = X,y                                #Making Input variables global.\n",
        "    self.x_t,self.y_t=[],[]\n",
        "    self.models = []\n",
        "    X = X.to_numpy()\n",
        "    y = y.to_numpy()\n",
        "    for i in range(self.n_estimators):                 #Iterates over n_estimators=10 \n",
        "      indexes = np.random.choice(len(X),round(self.samples*len(X)),replace = True)  #Gives random index from training set.\n",
        "      Xt,yt = X[indexes],y[indexes]                    #Assigns random samples for training\n",
        "      self.x_t.append(Xt)\n",
        "      self.y_t.append(yt)\n",
        "      mod = dtc()                                      #mod = dtc() which is DecisionTreeClassifier() used for training\n",
        "      a = mod.fit(Xt,yt)                               \n",
        "      self.models.append(a)                            #Making a list of trained models. \n",
        "    self.x_t,self.y_t=pd.Series(self.x_t),pd.Series(self.y_t)\n",
        "\n",
        "  def predict(self,X_test):                            #Used to Predict the output y_pred\n",
        "    self.y_pred = []\n",
        "    X_test = X_test.values\n",
        "    for i in range(len(X_test)):\n",
        "      pred = []\n",
        "      for j in self.models:\n",
        "        pred.append(j.predict(X_test[i].reshape(1,-1))[0])     #Predicting from each trained model.\n",
        "      real_pred=max(set(pred), key = pred.count)               #Taking Majority of prediction which is basis of Voting Classifier.\n",
        "      self.y_pred.append(real_pred)                            #Making list of outputs.\n",
        "\n",
        "    return np.array(self.y_pred)                               #Returning array of outputs.\n",
        "\n",
        "  def accuracies(self,X_test,y_test):                  #Used to give Accuracy of each tree for summarizing each tree performance numerically.\n",
        "      self.Accuracy={}                               \n",
        "      self.predicted = []\n",
        "      y_new = np.array(y_test)\n",
        "      for i,j in zip(range(1,self.n_estimators+1),self.models):\n",
        "          y_p =j.predict(X_test)\n",
        "          self.predicted.append(y_p)\n",
        "          x=0\n",
        "          for k in range(len(y_p)):\n",
        "            if y_p[k] == y_new[k]:                    \n",
        "              x += 1\n",
        "          self.Accuracy[i]= (x/len(y_p))                    #Making Dictionary of the accuracies\n",
        "      return self.Accuracy                                  #Returning the dictionary\n",
        "\n",
        "  def visual(self):                                    #Used for Summarizing each tree performance visually\n",
        "    plt.plot(self.Accuracy.keys(),self.Accuracy.values())\n",
        "    plt.xlabel('Tree Number')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Visual Summarization of Each Tree')"
      ],
      "metadata": {
        "id": "sjRtk6Q3k9OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For K-Means Clustering**"
      ],
      "metadata": {
        "id": "oLSp8DpmpFel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class kmeans_cluster:\n",
        "  def __init__(self,k,max_iters):            #Initializing variables to input, value of k to be given by user.\n",
        "    self.k = k\n",
        "    self.max_iters  = max_iters\n",
        "\n",
        "  def fit(self,X,init_centroid):             #Used to predict the labels and update the centroid, taking initial cluster point from user.                                \n",
        "    self.init_centroid = init_centroid\n",
        "    n_samples,n_features = X.shape\n",
        "    for i in range(self.max_iters):\n",
        "      print                             #Stop iterating when max_iter is reached.\n",
        "      self.centroids = np.zeros((self.k, n_features))\n",
        "      self.labels = []\n",
        "      for j in X:\n",
        "        dist = np.sqrt(((init_centroid - j)**2).sum(axis=1))     #Computing distance\n",
        "        self.labels.append(np.argmin(dist))                      #Setting label w.r.t minimum distance.\n",
        "      self.labels = np.array(self.labels)\n",
        "      for j in range(len(self.init_centroid)):\n",
        "        if np.sum(self.labels == j) > 0:\n",
        "          self.centroids[j] = np.mean(X[self.labels == j], axis=0)\n",
        "      if (self.init_centroid == self.centroids).all() == True:        #Stop iterating when cluster centers not are changing.\n",
        "        break\n",
        "      self.init_centroid = self.centroids.copy()                      #Updating centroid\n",
        "\n",
        "  def label(self):                                                    #Used to return label.\n",
        "    a = np.array(self.labels)\n",
        "    return a\n",
        "\n",
        "  def centroid(self):                                                 #Used for returning the centroids.\n",
        "    return self.centroids\n",
        "\n",
        "  def SSE(self,X,centroid):                                           #Used to compute Sum of squared Error.\n",
        "    sse = 0\n",
        "    for i in range(X.shape[0]):\n",
        "      sq_dist = []\n",
        "      for j in range(self.k):\n",
        "        b = np.sum((X[i]-centroid[j])**2)\n",
        "        sq_dist.append(b)\n",
        "      sse += min(sq_dist)\n",
        "    return sse"
      ],
      "metadata": {
        "id": "wW2aPxTCk9Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**For PCA**"
      ],
      "metadata": {
        "id": "hkmOM1ohpKOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class pca:\n",
        "  def __init__(self,n=None):                         #Constructor for initializng variables for input. \n",
        "    self.n = n                                       # n = No. of Components\n",
        "\n",
        "  def covmat(self,X_):                                #For Computing the Co-variance Matrix             \n",
        "    mean,std = [],[]\n",
        "    for i in range(X_.shape[1]):                 \n",
        "      mean.append(np.mean(X_.iloc[:,i],axis=0))       #Finding Mean of the column.\n",
        "      std.append(np.std(X_.iloc[:,i],axis=0))         #Finding Std. Deviation of the column.\n",
        "      a = (X_.iloc[:,i] - mean[i])\n",
        "      b = a/std[i]\n",
        "      X_.iloc[:,i] = b                                #Standardizing the DataFrame Columnwise.\n",
        "      self.matrix = (X_.T@X_)/(X_.shape[0]-1)\n",
        "\n",
        "    return self.matrix                               #Returning the Covariance Matrix.\n",
        "\n",
        "  def fit(self,X_):\n",
        "    eigvect = np.linalg.eig(self.matrix)[1]         #Computing EigenVectors using inbuilt\n",
        "    lis = []\n",
        "    eigenvalues,eigenvects = [],[]\n",
        "    for i in range(eigvect.shape[1]):\n",
        "        v = eigvect[:, i]\n",
        "        eigval = np.dot(self.matrix,v) / v           #Finding EigenValues from scratch.\n",
        "        eigvals = eigval[0]\n",
        "        eigenvalues.append(eigvals)\n",
        "        eigenvects.append(v)\n",
        "        lis.append((eigvals,v))                      #Made a list of lists having eigenvalues and eigenvectors. \n",
        "    lis = sorted(lis,reverse = True)\n",
        "    self.eigenvalues = np.array([i[0] for i in lis])\n",
        "    self.eigvects = np.array([i[1] for i in lis]).T       \n",
        "    P = np.array(X_)\n",
        "    if self.n is None:                               # If n = None there will be no change in no. of components.\n",
        "        self.principal_components = P@(self.eigvects)\n",
        "    else:\n",
        "        self.principal_components = P@(self.eigvects[:,:self.n])\n",
        "\n",
        "    return self.principal_components\n",
        "\n",
        "  def transform(self,X_):\n",
        "    mat = self.covmat(X_)\n",
        "    principal_comp = self.fit(X_)\n",
        "    return principal_comp"
      ],
      "metadata": {
        "id": "KiqdinVYmyr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MLP**"
      ],
      "metadata": {
        "id": "TinHcYbfOwoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "      def __init__(self, input_size, hidden_size, output_size, l_rate, func, method):\n",
        "          self.input_size = input_size\n",
        "          self.hidden_size = hidden_size\n",
        "          self.output_size = output_size\n",
        "          self.l_rate = l_rate\n",
        "          self.func = func\n",
        "          self.method = method\n",
        "      \n",
        "          if self.method == 'random':\n",
        "              self.w1 = (np.random.randn(self.input_size, self.hidden_size))\n",
        "              self.b1 = np.zeros((1, self.hidden_size))\n",
        "              self.w2 = (np.random.randn(self.hidden_size, self.output_size))\n",
        "              self.b2 = np.zeros((1, self.output_size))\n",
        "          elif self.method == 'zero':\n",
        "              self.w1 = np.zeros((self.input_size, self.hidden_size))\n",
        "              self.b1 = np.zeros((1, self.hidden_size))\n",
        "              self.w2 = np.zeros((self.hidden_size, self.output_size))\n",
        "              self.b2 = np.zeros((1, self.output_size))\n",
        "          elif self.method == 'constant':\n",
        "              self.w1 = np.full((self.input_size, self.hidden_size), 0.1)\n",
        "              self.b1 = np.full((1, self.hidden_size), 0.1)\n",
        "              self.w2 = np.full((self.hidden_size, self.output_size), 0.1)\n",
        "              self.b2 = np.full((1, self.output_size), 0.1)\n",
        "\n",
        "       def activation_func(self, X):\n",
        "        if self.func == \"sigmoid\":\n",
        "            return self.sigmoid(X)\n",
        "        elif self.func == \"relu\":\n",
        "            return self.relu(X)\n",
        "        elif self.func == \"tanh\":\n",
        "            return self.tanh(X)\n",
        "\n",
        "      def activation_func_derivative(self, X):\n",
        "        if self.func == \"sigmoid\":\n",
        "            return self.sigmoid_derivative(X)\n",
        "        elif self.func == \"relu\":\n",
        "            return self.relu_derivative(X)\n",
        "        elif self.func == \"tanh\":\n",
        "            return self.tanh_derivative(X)\n",
        "\n",
        "      def sigmoid(self,x):\n",
        "          return 1 / (1 + np.exp(-x))\n",
        "\n",
        "      def relu(self,x):\n",
        "          return np.maximum(0, x)\n",
        "\n",
        "      def tanh(self,x):\n",
        "          return np.tanh(x)\n",
        "\n",
        "      def relu_derivative(self,x):\n",
        "          return np.where(x > 0, 1, 0)\n",
        "\n",
        "      def tanh_derivative(self,x):\n",
        "          return 1 - np.tanh(x)**2\n",
        "\n",
        "      def sigmoid_derivative(self,x):\n",
        "          return x * (1 - x)\n",
        "\n",
        "      def forward(self, X):\n",
        "          self.hidden_layer = self.activation_func(np.dot(X, self.w1) + self.b1 )\n",
        "          self.output_layer = self.sigmoid(np.dot(self.hidden_layer, self.w2) + self.b2)\n",
        "\n",
        "      def backward(self, X, y):\n",
        "          output_error = y - self.output_layer[0]\n",
        "          output_delta = (np.array(output_error * self.sigmoid_derivative(self.output_layer[0]))).reshape(-1,1)\n",
        "          hidden_error = output_delta.dot(self.w2.T)\n",
        "          hidden_delta = hidden_error * self.activation_func_derivative(self.hidden_layer)\n",
        "\n",
        "          self.w2 += self.hidden_layer.T.dot(output_delta) * self.l_rate\n",
        "          self.w1 += X.T.dot(hidden_delta) * self.l_rate\n",
        "\n",
        "      def train(self, X, y, n_e):\n",
        "          for e in range(n_e):\n",
        "              self.forward(X)\n",
        "              self.backward(X, y)\n",
        "\n",
        "      def predict(self, X):\n",
        "          self.forward(X)\n",
        "          return np.round(self.output_layer)"
      ],
      "metadata": {
        "id": "OOj5B6KlO38F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}